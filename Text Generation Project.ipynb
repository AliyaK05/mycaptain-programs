{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AliyaK05/mycaptain-programs/blob/main/Text%20Generation%20Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QX-WrBqzUX2",
        "outputId": "f2d9246f-9c5a-4d75-dde7-ad5c181fb3f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "# import dependencies\n",
        "import numpy\n",
        "import sys\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, LSTM\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import ModelCheckpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1bRZSmXx5FB0"
      },
      "outputs": [],
      "source": [
        "# load data and opening data in form of text file - taken from project gutenberg site\n",
        "import requests\n",
        "result=requests.get('https://www.gutenberg.org/cache/epub/84/pg84.txt')\n",
        "file = result.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OIllIX1tykkE"
      },
      "outputs": [],
      "source": [
        "# import dependencies\n",
        "import numpy\n",
        "import sys\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, LSTM\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import ModelCheckpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NXVcaCGgykkF"
      },
      "outputs": [],
      "source": [
        "# tokenization-breaking up a stream of text into words,phrases, symbols or meaninful elements\n",
        "# standardization\n",
        "def tokenize_words(input):\n",
        "  input = input.lower()\n",
        "  tokenizer = RegexpTokenizer(r'\\w+')\n",
        "  tokens = tokenizer.tokenize(input)\n",
        "  filtered = filter(lambda token: token not in stopwords.words('english'), tokens)\n",
        "  return \"\".join(filtered)\n",
        "\n",
        "p_inputs = tokenize_words(file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "b5G3pehpykkF"
      },
      "outputs": [],
      "source": [
        "# chars to numbers\n",
        "chars = sorted(list(set(p_inputs)))\n",
        "char_to_num = dict((c,i) for i, c in enumerate(chars))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yz1AB5bVykkF",
        "outputId": "28b73758-e81d-459a-debe-f3f609695cb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of characters:  233002\n",
            "Total vocab:  42\n"
          ]
        }
      ],
      "source": [
        "# check if words to chars or chars to num(?!) has worked?\n",
        "input_len = len(p_inputs)\n",
        "vocab_len = len(chars)\n",
        "print(\"Total number of characters: \", input_len)\n",
        "print(\"Total vocab: \", vocab_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1OsRmFY-ykkF"
      },
      "outputs": [],
      "source": [
        "# seq length\n",
        "seq_length = 100\n",
        "x_data = []\n",
        "y_data = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59WE8J3UykkG",
        "outputId": "c612772c-0750-4837-9ac2-a25b0f65c338"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Patterns: 232902\n"
          ]
        }
      ],
      "source": [
        "# loop through the sequence\n",
        "for i in range(0, input_len - seq_length, 1):\n",
        "  in_seq = p_inputs[i:i + seq_length]\n",
        "  out_seq = p_inputs[i + seq_length]\n",
        "  x_data.append([char_to_num[char] for char in in_seq])\n",
        "  y_data.append(char_to_num[out_seq])\n",
        "\n",
        "n_patterns = len(x_data)\n",
        "print(\"Total Patterns:\", n_patterns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "F9vJ2HD1ykkG"
      },
      "outputs": [],
      "source": [
        "# convert input sequence to np array and so on\n",
        "X = numpy.reshape(x_data, (n_patterns, seq_length, 1))\n",
        "X = X/float(vocab_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "3qZ_UeGVykkG"
      },
      "outputs": [],
      "source": [
        "# one-hot encoding\n",
        "y = np_utils.to_categorical(y_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "3rJgOThMykkG"
      },
      "outputs": [],
      "source": [
        "# creating the model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "bzgbCFAJykkG"
      },
      "outputs": [],
      "source": [
        "# compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "VXKV7W57ykkG"
      },
      "outputs": [],
      "source": [
        "# saving weights\n",
        "filepath = 'model_weights_saved.hdf5'\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose = 1, save_best_only=True, mode='min')\n",
        "desired_callbacks = [checkpoint]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeDd_-JHykkG",
        "outputId": "9f5fa59d-6aed-437d-c8b5-1753a898cdd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "910/910 [==============================] - ETA: 0s - loss: 2.9363\n",
            "Epoch 1: loss improved from inf to 2.93635, saving model to model_weights_saved.hdf5\n",
            "910/910 [==============================] - 3671s 4s/step - loss: 2.9363\n",
            "Epoch 2/4\n",
            "910/910 [==============================] - ETA: 0s - loss: 2.9153\n",
            "Epoch 2: loss improved from 2.93635 to 2.91525, saving model to model_weights_saved.hdf5\n",
            "910/910 [==============================] - 3634s 4s/step - loss: 2.9153\n",
            "Epoch 3/4\n",
            "910/910 [==============================] - ETA: 0s - loss: 2.9120\n",
            "Epoch 3: loss improved from 2.91525 to 2.91198, saving model to model_weights_saved.hdf5\n",
            "910/910 [==============================] - 3632s 4s/step - loss: 2.9120\n",
            "Epoch 4/4\n",
            "910/910 [==============================] - ETA: 0s - loss: 2.8886\n",
            "Epoch 4: loss improved from 2.91198 to 2.88858, saving model to model_weights_saved.hdf5\n",
            "910/910 [==============================] - 3670s 4s/step - loss: 2.8886\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7da0ef20a4a0>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# fit model and let it train\n",
        "model.fit(X, y, epochs=4, batch_size=256, callbacks=desired_callbacks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "gRYSR69WykkG"
      },
      "outputs": [],
      "source": [
        "# recompile model with the saved weights\n",
        "filename = 'model_weights_saved.hdf5'\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "RND3lvyTykkH"
      },
      "outputs": [],
      "source": [
        "# output of the model back into characters\n",
        "num_to_char = dict((i,c) for i,c in enumerate(chars))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "H0PEtpXzykkH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c1e595d-c829-4e6e-f4df-010aee004838"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Seed:\n",
            "p\".p\".e\".a\".c\".e\".t\".h\".i\".n\".k\".s\".s\".u\".r\".e\".l\".y\".t\".h\".i\".n\".k\".t\".h\".u\".s\".f\".a\".r\".e\".w\".e\".l\".l\".s\".p\".r\".a\".n\".g\".c\".a\".b\".i\".n\".w\".i\".n\".d\".o\".w\".s\".a\".i\".d\".u\".p\".o\".n\".i\".c\".e\".r\".a\".f\".t\".l\".a\".y\".c\".l\".o\".s\".e\".v\".e\".s\".s\".e\".l\".s\".o\".o\".n\".b\".o\".r\".n\".e\".a\".w\".a\".y\".w\".a\".v\".e\".s\".l \"\n"
          ]
        }
      ],
      "source": [
        "# random seed to help generate\n",
        "start = numpy.random.randint(0, len(x_data) - 1)\n",
        "pattern = x_data[start]\n",
        "print(\"Random Seed:\")\n",
        "print(\"\\\".\".join([num_to_char[value] for value in pattern]), \"\\\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "U7Z2l8fyykkH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb5e177d-a606-4714-d469-7d0ed2563715"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee"
          ]
        }
      ],
      "source": [
        "# generate the text\n",
        "for i in range(1000):\n",
        "  x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "  x = x/float(vocab_len)\n",
        "  prediction = model.predict(x, verbose=0)\n",
        "  index = numpy.argmax(prediction)\n",
        "  result = num_to_char[index]\n",
        "  seg_in = [num_to_char[value] for value in pattern]\n",
        "  sys.stdout.write(result)\n",
        "  pattern.append(index)\n",
        "  pattern = pattern[1:len(pattern)]\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPk4dKNzcRcNOros5oIh/L8",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}